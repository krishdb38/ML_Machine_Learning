{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 오토인코더 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder:\n",
    "    def __init__(self, input_dim, hidden_dim, epoch=250, learning_rate=0.001):\n",
    "        self.epoch = epoch # 학습 횟수\n",
    "        self.learning_rate = learning_rate # 최적화기의 하이퍼파라미터\n",
    "        \n",
    "        x = tf.placeholder(dtype=tf.float32, shape=[None, input_dim]) # 입력 레이어 데이터셋 정의\n",
    "        \n",
    "        with tf.name_scope('encode'): # 이름 범위 아래에서 가중치와 편향을 정의함으로써 디코더의 가중치와 편향으로부터 구분할 수 있습니다\n",
    "            weights = tf.Variable(tf.random_normal([input_dim, hidden_dim], dtype=tf.float32), name='weights')\n",
    "            biases = tf.Variable(tf.zeros([hidden_dim]), name='biases')\n",
    "            encoded = tf.nn.tanh(tf.matmul(x, weights) + biases)\n",
    "        with tf.name_scope('decode'): # 디코더의 가중치와 편향은 이 이름 범위 하에서 정의됩니다\n",
    "            weights = tf.Variable(tf.random_normal([hidden_dim, input_dim], dtype=tf.float32), name='weights')\n",
    "            biases = tf.Variable(tf.zeros([input_dim]), name='biases')\n",
    "            decoded = tf.matmul(encoded, weights) + biases\n",
    "            \n",
    "        self.x = x # 메소드 변수들입니다\n",
    "        self.encoded = encoded\n",
    "        self.decoded = decoded\n",
    "        \n",
    "        self.loss = tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(self.x, self.decoded)))) # 재현 비용을 정의합니다\n",
    "        self.train_op = tf.train.RMSPropOptimizer(self.learning_rate).minimize(self.loss) # 최적화기를 선택합니다\n",
    "        self.saver = tf.train.Saver() # 학습을 진행하면서 모델 파라미터들을 저장하기 위해 saver를 설정합니다\n",
    "        \n",
    "    def train(self, data):\n",
    "        num_samples = len(data)\n",
    "        with tf.Session() as sess: # 텐서플로우 세션을 시작하고 모든 변수들을 초기화합니다\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            for i in range(self.epoch): # 생성자에 정의된 수만큼 반복합니다\n",
    "                for j in range(num_samples): # 한번에 하나씩 데이터 항목을 이용해 신경망을 학습시킵니다\n",
    "                    l, _ = sess.run([self.loss, self.train_op], feed_dict={self.x: [data[j]]})\n",
    "                if i % 10 == 0:\n",
    "                    print('epoch {0}: loss = {1}'.format(i, l)) # 10번마다 출력해 줍니다\n",
    "                    self.saver.save(sess, './model.ckpt') # 학습된 파라미터를 파일로 저장합니다\n",
    "            self.saver.save(sess, './model.ckpt')\n",
    "            \n",
    "    def test(self, data):\n",
    "        with tf.Session() as sess:\n",
    "            self.saver.restore(sess, './model.ckpt') # 학습된 파라미터를 불러옵니다\n",
    "            hidden, reconstructed = sess.run([self.encoded, self.decoded],feed_dict={self.x: data}) # 입력을 복원해 줍니다\n",
    "        print('input', data)\n",
    "        print('compressed', hidden)\n",
    "        print('reconstructed', reconstructed)\n",
    "        return reconstructed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.6 Autoencoder 클래스 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: loss = 4.762742519378662\n",
      "epoch 10: loss = 1.944270372390747\n",
      "epoch 20: loss = 0.8767220973968506\n",
      "epoch 30: loss = 0.4848218262195587\n",
      "epoch 40: loss = 0.41396087408065796\n",
      "epoch 50: loss = 0.4091811776161194\n",
      "epoch 60: loss = 0.4066024720668793\n",
      "epoch 70: loss = 0.4022473096847534\n",
      "epoch 80: loss = 0.395643413066864\n",
      "epoch 90: loss = 0.3860918879508972\n",
      "epoch 100: loss = 0.3730546534061432\n",
      "epoch 110: loss = 0.35694658756256104\n",
      "epoch 120: loss = 0.33847108483314514\n",
      "epoch 130: loss = 0.31794899702072144\n",
      "epoch 140: loss = 0.2952507436275482\n",
      "epoch 150: loss = 0.27193784713745117\n",
      "epoch 160: loss = 0.2523600459098816\n",
      "epoch 170: loss = 0.24104058742523193\n",
      "epoch 180: loss = 0.23876768350601196\n",
      "epoch 190: loss = 0.24279622733592987\n",
      "epoch 200: loss = 0.2499697059392929\n",
      "epoch 210: loss = 0.2581237554550171\n",
      "epoch 220: loss = 0.2660641372203827\n",
      "epoch 230: loss = 0.27346184849739075\n",
      "epoch 240: loss = 0.2804086208343506\n",
      "INFO:tensorflow:Restoring parameters from ./model.ckpt\n",
      "input [[8, 4, 6, 2]]\n",
      "compressed [[-0.8872787]]\n",
      "reconstructed [[6.401917  2.8301592 5.366955  1.8683255]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[6.401917 , 2.8301592, 5.366955 , 1.8683255]], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_dim = 1\n",
    "data = datasets.load_iris().data\n",
    "input_dim = len(data[0])\n",
    "ae = Autoencoder(input_dim, hidden_dim)\n",
    "ae.train(data)\n",
    "ae.test([[8, 4, 6, 2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.7 배치 도우미 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(X, size):\n",
    "    a = np.random.choice(len(X), size, replace=False)\n",
    "    return X[a]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.8 배치 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder_batch:\n",
    "    def __init__(self, input_dim, hidden_dim, epoch=250, learning_rate=0.001):\n",
    "        self.epoch = epoch # 학습 횟수\n",
    "        self.learning_rate = learning_rate # 최적화기의 하이퍼파라미터\n",
    "        \n",
    "        x = tf.placeholder(dtype=tf.float32, shape=[None, input_dim]) # 입력 레이어 데이터셋 정의\n",
    "        \n",
    "        with tf.name_scope('encode'): # 이름 범위 아래에서 가중치와 편향을 정의함으로써 디코더의 가중치와 편향으로부터 구분할 수 있습니다\n",
    "            weights = tf.Variable(tf.random_normal([input_dim, hidden_dim], dtype=tf.float32), name='weights')\n",
    "            biases = tf.Variable(tf.zeros([hidden_dim]), name='biases')\n",
    "            encoded = tf.nn.tanh(tf.matmul(x, weights) + biases)\n",
    "        with tf.name_scope('decode'): # 디코더의 가중치와 편향은 이 이름 범위 하에서 정의됩니다\n",
    "            weights = tf.Variable(tf.random_normal([hidden_dim, input_dim], dtype=tf.float32), name='weights')\n",
    "            biases = tf.Variable(tf.zeros([input_dim]), name='biases')\n",
    "            decoded = tf.matmul(encoded, weights) + biases\n",
    "            \n",
    "        self.x = x # 메소드 변수들입니다\n",
    "        self.encoded = encoded\n",
    "        self.decoded = decoded\n",
    "        \n",
    "        self.loss = tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(self.x, self.decoded)))) # 재현 비용을 정의합니다\n",
    "        self.train_op = tf.train.RMSPropOptimizer(self.learning_rate).minimize(self.loss) # 최적화기를 선택합니다\n",
    "        self.saver = tf.train.Saver() # 학습을 진행하면서 모델 파라미터들을 저장하기 위해 saver를 설정합니다\n",
    "        \n",
    "    def train(self, data, batch_size=10):\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            for i in range(self.epoch):\n",
    "                for j in range(500): # 배치만큼 반복하게 됩니다\n",
    "                    batch_data = get_batch(data, batch_size) # 랜덤하게 선택된 배치에 대해 최적화기를 실행합니다\n",
    "                    l, _ = sess.run([self.loss, self.train_op],feed_dict={self.x: batch_data})\n",
    "                if i % 10 == 0:\n",
    "                    print('epoch {0}: loss = {1}'.format(i, l))\n",
    "                    self.saver.save(sess, './model.ckpt')\n",
    "            self.saver.save(sess, './model.ckpt')\n",
    "            \n",
    "    def test(self, data):\n",
    "        with tf.Session() as sess:\n",
    "            self.saver.restore(sess, './model.ckpt') # 학습된 파라미터를 불러옵니다\n",
    "            hidden, reconstructed = sess.run([self.encoded, self.decoded],feed_dict={self.x: data}) # 입력을 복원해 줍니다\n",
    "        print('input', data)\n",
    "        print('compressed', hidden)\n",
    "        print('reconstructed', reconstructed)\n",
    "        return reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: loss = 2.9653894901275635\n",
      "epoch 10: loss = 0.6367222666740417\n",
      "epoch 20: loss = 0.4280121922492981\n",
      "epoch 30: loss = 0.4102773666381836\n",
      "epoch 40: loss = 0.34543317556381226\n",
      "epoch 50: loss = 0.3505767285823822\n",
      "epoch 60: loss = 0.230178564786911\n",
      "epoch 70: loss = 0.3078746795654297\n",
      "epoch 80: loss = 0.23413164913654327\n",
      "epoch 90: loss = 0.250286340713501\n",
      "epoch 100: loss = 0.3602640628814697\n",
      "epoch 110: loss = 0.3141750991344452\n",
      "epoch 120: loss = 0.30150073766708374\n",
      "epoch 130: loss = 0.20863670110702515\n",
      "epoch 140: loss = 0.30766263604164124\n",
      "epoch 150: loss = 0.25836998224258423\n",
      "epoch 160: loss = 0.3910287022590637\n",
      "epoch 170: loss = 0.24819539487361908\n",
      "epoch 180: loss = 0.2692684531211853\n",
      "epoch 190: loss = 0.2519698441028595\n",
      "epoch 200: loss = 0.3069063723087311\n",
      "epoch 210: loss = 0.29161790013313293\n",
      "epoch 220: loss = 0.2231282740831375\n",
      "epoch 230: loss = 0.23868906497955322\n",
      "epoch 240: loss = 0.4563460350036621\n",
      "INFO:tensorflow:Restoring parameters from ./model.ckpt\n",
      "input [[8, 4, 6, 2]]\n",
      "compressed [[0.4180975]]\n",
      "reconstructed [[6.8641934 2.8006907 6.1956196 2.2268982]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[6.8641934, 2.8006907, 6.1956196, 2.2268982]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_dim = 1\n",
    "data = datasets.load_iris().data\n",
    "input_dim = len(data[0])\n",
    "ae = Autoencoder_batch(input_dim, hidden_dim)\n",
    "ae.train(data)\n",
    "ae.test([[8, 4, 6, 2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.10 추출된 CIFAR-10 데이터셋으로부터 읽어오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.misc import imread, imresize\n",
    "import pickle\n",
    "\n",
    "def unpickle(file): # CIFAR-10 파일을 읽어와서 로드된 사전을 반환합니다\n",
    "    fo = open(file, 'rb')\n",
    "    dict = pickle.load(fo, encoding='latin1')\n",
    "    fo.close()\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.11 모든 CIFAR-10 파일을 읽어 메모리에 저장하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = unpickle('./cifar-10-batches-py/batches.meta')['label_names']\n",
    "data, labels = [], []\n",
    "for i in range(1, 6): # 6개의 파일에 대해 반복합니다\n",
    "    filename = './cifar-10-batches-py/data_batch_' + str(i)\n",
    "    batch_data = unpickle(filename) # 파일을 로드하여 파이썬 사전을 획득합니다\n",
    "    if len(data) > 0:\n",
    "        data = np.vstack((data, batch_data['data'])) # 데이터 샘플의 행은 각각의 샘플을 표현하기 때문에 수직으로 쌓게 됩니다\n",
    "        labels = np.hstack((labels, batch_data['labels'])) # 레이블은 1차원이기 때문에 수평으로 쌓게 됩니다\n",
    "    else:\n",
    "        data = batch_data['data']\n",
    "        labels = batch_data['labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.12 CIFAR-10 이미지를 흑백으로 변환하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grayscale(a):\n",
    "    return a.reshape(a.shape[0], 3, 32, 32).mean(1).reshape(a.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = grayscale(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 1024)\n",
      "epoch 0: loss = 89.40890502929688\n",
      "epoch 10: loss = 59.64217758178711\n",
      "epoch 20: loss = 47.695709228515625\n",
      "epoch 30: loss = 42.410377502441406\n",
      "epoch 40: loss = 48.668739318847656\n",
      "epoch 50: loss = 56.84235382080078\n",
      "epoch 60: loss = 45.0015983581543\n",
      "epoch 70: loss = 44.886409759521484\n",
      "epoch 80: loss = 50.54345703125\n",
      "epoch 90: loss = 49.140350341796875\n",
      "epoch 100: loss = 51.03417205810547\n",
      "epoch 110: loss = 50.56010818481445\n",
      "epoch 120: loss = 57.118656158447266\n",
      "epoch 130: loss = 47.41543197631836\n",
      "epoch 140: loss = 54.57421875\n",
      "epoch 150: loss = 46.37538528442383\n",
      "epoch 160: loss = 51.31253433227539\n",
      "epoch 170: loss = 47.668418884277344\n",
      "epoch 180: loss = 49.60804748535156\n",
      "epoch 190: loss = 43.20268249511719\n",
      "epoch 200: loss = 42.5274772644043\n",
      "epoch 210: loss = 48.62804412841797\n",
      "epoch 220: loss = 49.7543830871582\n",
      "epoch 230: loss = 50.54779815673828\n",
      "epoch 240: loss = 42.87990951538086\n"
     ]
    }
   ],
   "source": [
    "x = np.matrix(data)\n",
    "y = np.array(labels)\n",
    "\n",
    "horse_indices = np.where(y == 7)[0]\n",
    "horse_x = x[horse_indices]\n",
    "\n",
    "print(np.shape(horse_x))  # (5000, 3072)\n",
    "\n",
    "input_dim = np.shape(horse_x)[1]\n",
    "hidden_dim = 100\n",
    "\n",
    "ae = Autoencoder_batch(input_dim, hidden_dim)\n",
    "ae.train(horse_x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
