{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf # 관련 라이브러리를 불러오고 하이퍼파라미터를 초기화합니다\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "training_epochs = 1000\n",
    "reg_lambda = 0.\n",
    "\n",
    "x_dataset = np.linspace(-1, 1, 100) # 가짜 데이터셋인 y = x2 을 생성합니다\n",
    "\n",
    "num_coeffs = 9\n",
    "y_dataset_params = [0.] * num_coeffs\n",
    "y_dataset_params[2] = 1\n",
    "y_dataset = 0\n",
    "for i in range(num_coeffs):\n",
    "    y_dataset += y_dataset_params[i] * np.power(x_dataset, i)\n",
    "y_dataset += np.random.randn(*x_dataset.shape) * 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(x_dataset, y_dataset, ratio): # 입력 데이터셋, 출력 데이터셋, 분리 비율을 인자로 받게 됩니다\n",
    "    arr = np.arange(x_dataset.size) # 리스트의 값들을 섞어 줍니다\n",
    "    np.random.shuffle(arr)\n",
    "    num_train = int(ratio * x_dataset.size) # 학습 데이터의 개수를 계산합니다\n",
    "    x_train = x_dataset[arr[0:num_train]] # 섞어준 리스트를 이용하여 x_dataset 을 분리합니다\n",
    "    x_test = x_dataset[arr[num_train:x_dataset.size]]\n",
    "    y_train = y_dataset[arr[0:num_train]] # 섞어준 리스트를 이용하여 y_dataset 을 분리합니다\n",
    "    y_test = y_dataset[arr[num_train:x_dataset.size]]\n",
    "    return x_train, x_test, y_train, y_test # 분리된 x 데이터셋과 y 데이터셋을 반환합니다\n",
    "\n",
    "(x_train, x_test, y_train, y_test) = split_dataset(x_dataset, y_dataset, 0.7) # 코드 3.4에서 만들어둔 함수를 호출하여 데이터셋을 학습 70%, 테스트 30%로 분리합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32) # 입력/출력 플레이스홀더를 설정합니다\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "def model(X, w): # 모델을 정의합니다\n",
    "    terms = []\n",
    "    for i in range(num_coeffs):\n",
    "        term = tf.multiply(w[i], tf.pow(X, i))\n",
    "        terms.append(term)\n",
    "    return tf.add_n(terms)\n",
    "\n",
    "w = tf.Variable([0.] * num_coeffs, name=\"parameters\") # 정규화된 비용 함수를 정의합니다\n",
    "y_model = model(X, w)\n",
    "cost = tf.div(tf.add(tf.reduce_sum(tf.square(Y-y_model)), \n",
    "                     tf.multiply(reg_lambda, tf.reduce_sum(tf.square(w)))),\n",
    "              2*x_train.size)\n",
    "train_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reg lambda 0.0\n",
      "final cost 0.027053397\n",
      "reg lambda 0.010101010101010102\n",
      "final cost 0.02100246\n",
      "reg lambda 0.020202020202020204\n",
      "final cost 0.01914049\n",
      "reg lambda 0.030303030303030304\n",
      "final cost 0.018233152\n",
      "reg lambda 0.04040404040404041\n",
      "final cost 0.017693834\n",
      "reg lambda 0.05050505050505051\n",
      "final cost 0.017341815\n",
      "reg lambda 0.06060606060606061\n",
      "final cost 0.017097484\n",
      "reg lambda 0.07070707070707072\n",
      "final cost 0.01691852\n",
      "reg lambda 0.08080808080808081\n",
      "final cost 0.016780263\n",
      "reg lambda 0.09090909090909091\n",
      "final cost 0.016667718\n",
      "reg lambda 0.10101010101010102\n",
      "final cost 0.01657163\n",
      "reg lambda 0.11111111111111112\n",
      "final cost 0.016486261\n",
      "reg lambda 0.12121212121212122\n",
      "final cost 0.016408082\n",
      "reg lambda 0.13131313131313133\n",
      "final cost 0.016334927\n",
      "reg lambda 0.14141414141414144\n",
      "final cost 0.016265487\n",
      "reg lambda 0.15151515151515152\n",
      "final cost 0.016198978\n",
      "reg lambda 0.16161616161616163\n",
      "final cost 0.016134933\n",
      "reg lambda 0.17171717171717174\n",
      "final cost 0.016073078\n",
      "reg lambda 0.18181818181818182\n",
      "final cost 0.016013265\n",
      "reg lambda 0.19191919191919193\n",
      "final cost 0.015955389\n",
      "reg lambda 0.20202020202020204\n",
      "final cost 0.015899407\n",
      "reg lambda 0.21212121212121213\n",
      "final cost 0.015845258\n",
      "reg lambda 0.22222222222222224\n",
      "final cost 0.015792912\n",
      "reg lambda 0.23232323232323235\n",
      "final cost 0.015742352\n",
      "reg lambda 0.24242424242424243\n",
      "final cost 0.015693497\n",
      "reg lambda 0.25252525252525254\n",
      "final cost 0.01564635\n",
      "reg lambda 0.26262626262626265\n",
      "final cost 0.015600872\n",
      "reg lambda 0.27272727272727276\n",
      "final cost 0.01555703\n",
      "reg lambda 0.2828282828282829\n",
      "final cost 0.01551473\n",
      "reg lambda 0.29292929292929293\n",
      "final cost 0.015473996\n",
      "reg lambda 0.30303030303030304\n",
      "final cost 0.015434679\n",
      "reg lambda 0.31313131313131315\n",
      "final cost 0.015396874\n",
      "reg lambda 0.32323232323232326\n",
      "final cost 0.015360398\n",
      "reg lambda 0.33333333333333337\n",
      "final cost 0.015325304\n",
      "reg lambda 0.3434343434343435\n",
      "final cost 0.015291483\n",
      "reg lambda 0.3535353535353536\n",
      "final cost 0.015258956\n",
      "reg lambda 0.36363636363636365\n",
      "final cost 0.015227579\n",
      "reg lambda 0.37373737373737376\n",
      "final cost 0.015197449\n",
      "reg lambda 0.38383838383838387\n",
      "final cost 0.015168381\n",
      "reg lambda 0.393939393939394\n",
      "final cost 0.015140455\n",
      "reg lambda 0.4040404040404041\n",
      "final cost 0.015113609\n",
      "reg lambda 0.4141414141414142\n",
      "final cost 0.015087732\n",
      "reg lambda 0.42424242424242425\n",
      "final cost 0.015062804\n",
      "reg lambda 0.43434343434343436\n",
      "final cost 0.015038909\n",
      "reg lambda 0.4444444444444445\n",
      "final cost 0.0150158955\n",
      "reg lambda 0.4545454545454546\n",
      "final cost 0.014993743\n",
      "reg lambda 0.4646464646464647\n",
      "final cost 0.014972432\n",
      "reg lambda 0.4747474747474748\n",
      "final cost 0.01495202\n",
      "reg lambda 0.48484848484848486\n",
      "final cost 0.014932404\n",
      "reg lambda 0.494949494949495\n",
      "final cost 0.01491353\n",
      "reg lambda 0.5050505050505051\n",
      "final cost 0.014895388\n",
      "reg lambda 0.5151515151515152\n",
      "final cost 0.01487795\n",
      "reg lambda 0.5252525252525253\n",
      "final cost 0.014861202\n",
      "reg lambda 0.5353535353535354\n",
      "final cost 0.014845124\n",
      "reg lambda 0.5454545454545455\n",
      "final cost 0.014829697\n",
      "reg lambda 0.5555555555555556\n",
      "final cost 0.014814929\n",
      "reg lambda 0.5656565656565657\n",
      "final cost 0.01480076\n",
      "reg lambda 0.5757575757575758\n",
      "final cost 0.014787168\n",
      "reg lambda 0.5858585858585859\n",
      "final cost 0.014774139\n",
      "reg lambda 0.595959595959596\n",
      "final cost 0.014761659\n",
      "reg lambda 0.6060606060606061\n",
      "final cost 0.014749704\n",
      "reg lambda 0.6161616161616162\n",
      "final cost 0.014738248\n",
      "reg lambda 0.6262626262626263\n",
      "final cost 0.014727294\n",
      "reg lambda 0.6363636363636365\n",
      "final cost 0.014716814\n",
      "reg lambda 0.6464646464646465\n",
      "final cost 0.014706786\n",
      "reg lambda 0.6565656565656566\n",
      "final cost 0.014697204\n",
      "reg lambda 0.6666666666666667\n",
      "final cost 0.014688046\n",
      "reg lambda 0.6767676767676768\n",
      "final cost 0.014679313\n",
      "reg lambda 0.686868686868687\n",
      "final cost 0.0146710025\n",
      "reg lambda 0.696969696969697\n",
      "final cost 0.0146630965\n",
      "reg lambda 0.7070707070707072\n",
      "final cost 0.014655565\n",
      "reg lambda 0.7171717171717172\n",
      "final cost 0.014648386\n",
      "reg lambda 0.7272727272727273\n",
      "final cost 0.014641568\n",
      "reg lambda 0.7373737373737375\n",
      "final cost 0.014635067\n",
      "reg lambda 0.7474747474747475\n",
      "final cost 0.014628869\n",
      "reg lambda 0.7575757575757577\n",
      "final cost 0.014623009\n",
      "reg lambda 0.7676767676767677\n",
      "final cost 0.014617503\n",
      "reg lambda 0.7777777777777778\n",
      "final cost 0.014612284\n",
      "reg lambda 0.787878787878788\n",
      "final cost 0.014607324\n",
      "reg lambda 0.797979797979798\n",
      "final cost 0.014602605\n",
      "reg lambda 0.8080808080808082\n",
      "final cost 0.014598186\n",
      "reg lambda 0.8181818181818182\n",
      "final cost 0.014594076\n",
      "reg lambda 0.8282828282828284\n",
      "final cost 0.014590187\n",
      "reg lambda 0.8383838383838385\n",
      "final cost 0.014586505\n",
      "reg lambda 0.8484848484848485\n",
      "final cost 0.014583036\n",
      "reg lambda 0.8585858585858587\n",
      "final cost 0.014579896\n",
      "reg lambda 0.8686868686868687\n",
      "final cost 0.014576928\n",
      "reg lambda 0.8787878787878789\n",
      "final cost 0.014574116\n",
      "reg lambda 0.888888888888889\n",
      "final cost 0.014571556\n",
      "reg lambda 0.8989898989898991\n",
      "final cost 0.014569225\n",
      "reg lambda 0.9090909090909092\n",
      "final cost 0.014567023\n",
      "reg lambda 0.9191919191919192\n",
      "final cost 0.0145650115\n",
      "reg lambda 0.9292929292929294\n",
      "final cost 0.014563268\n",
      "reg lambda 0.9393939393939394\n",
      "final cost 0.014561606\n",
      "reg lambda 0.9494949494949496\n",
      "final cost 0.01456007\n",
      "reg lambda 0.9595959595959597\n",
      "final cost 0.014558799\n",
      "reg lambda 0.9696969696969697\n",
      "final cost 0.014557624\n",
      "reg lambda 0.9797979797979799\n",
      "final cost 0.014556613\n",
      "reg lambda 0.98989898989899\n",
      "final cost 0.0145558\n",
      "reg lambda 1.0\n",
      "final cost 0.014554988\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session() # 세션을 설정합니다\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "for reg_lambda in np.linspace(0, 1, 100): # 다양한 정규화 파라미터를 시도해 봅니다\n",
    "    for epoch in range(training_epochs):\n",
    "        sess.run(train_op, feed_dict={X: x_train, Y: y_train})\n",
    "    final_cost = sess.run(cost, feed_dict={X: x_test, Y: y_test})\n",
    "    print('reg lambda', reg_lambda)\n",
    "    print('final cost', final_cost)\n",
    "\n",
    "sess.close() # 세션을 닫습니다"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
